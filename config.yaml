# RAG Cream PPO Training Configuration

# Device configuration
device: "cuda:0"  # Use GPU for training

# Retriever configuration
retriever:
  # Dataset options:
  # - "data/corpus.jsonl" (original: 70K mixed documents)
  # - "data/enhanced_corpus.jsonl" (original + 10K Wikipedia articles)
  # - "data/wikipedia_corpus.jsonl" (50K Wikipedia articles only)
  document_path: "data/combined/articles.jsonl"  # Path to your document corpus
  embedding_path: "index_embeddings/doc_embeddings.npy"  # Path to precomputed embeddings
  top_k: 5  # Number of documents to retrieve

# Generator (LLM) configuration
generator:
  model: "meta-llama/Llama-2-7b-chat-hf"  # Your LLaMA model path
  max_new_tokens: 256
  temperature: 0.7
  use_8bit: false  # Disable quantization for stability
  use_4bit: false  # Disable 4-bit for better performance
  use_flash_attention: true  # Enable flash attention for A100 performance
  gradient_checkpointing: true  # Enable gradient checkpointing for memory efficiency

# Reward model configuration
reward_model:
  model: "meta-llama/Llama-2-7b-chat-hf"  # Can be same as generator
  gradient_checkpointing: true  # Enable gradient checkpointing for memory efficiency

# Enhanced PPO Training configuration with Consistency Rewards
training:
  # Basic parameters - Optimized for speed
  epochs: 3  # Fewer epochs for faster training
  learning_rate: 5e-6  # Balanced learning rate
  batch_size: 1  # Smaller batch for faster processing
  max_grad_norm: 0.5  # Standard value
  
  # Model parameters
  max_input_length: 64  # Smaller for faster generation
  max_new_tokens: 8  # Smaller for faster generation
  temperature: 0.7
  
  # PPO-specific hyperparameters
  clip_range: 0.1         # Standard PPO clip range
  vf_coef: 0.5            # Standard value function coefficient
  ent_coef: 0.01          # Standard entropy coefficient
  gamma: 0.99              # Discount factor
  gae_lambda: 0.95         # GAE lambda parameter
  ppo_epochs: 1            # Single epoch for stability
  minibatch_size: 1        # Single sample for stability
  target_kl: 0.05          # Reasonable target KL
  
  # Value function settings
  normalize_advantages: true    # Whether to normalize advantages
  value_clip: true             # Whether to clip value function loss
  value_lr_multiplier: 1.0     # Learning rate multiplier for value head
  adaptive_kl: true            # Use adaptive KL control
  
  # Consistency reward settings
  lambda_consistency: 0.5      # Weight for consistency reward
  lambda_retrieval: 0.1        # Weight for retrieval consistency
  consistency_method: "spearman"  # Consistency method: "spearman", "kendall", "toporder"
  num_candidates: 3            # Number of candidates for consistency evaluation
  
  # Training stability settings
  use_gae: true               # Use Generalized Advantage Estimation
  normalize_rewards: true     # Normalize rewards using running statistics
  reward_clipping: true       # Clip rewards to prevent extreme values
  reward_clip_range: 10.0     # Range for reward clipping
  
  # Loss weights (legacy - now handled by lambda_consistency and lambda_retrieval)
  lambda_reward: 1.0
  
  # Training strategies
  use_direct_document_training: true
  direct_training_ratio: 0.3
  top_k_variance: true
  
  # Optional: separate questions file
  questions_path: "data/combined/questions.jsonl"
  
  # Data parameters
  max_questions_from_corpus: 50  # Much smaller for faster training
  context_length_limit: 200
  document_truncation_limit: 100
  
  # Optimizer settings
  optimizer: "adamw"  # or "adam"
  weight_decay: 0.01
  scheduler: "cosine"  # or "linear" or null
  
  # Logging and saving
  log_interval: 10
  eval_interval: 100
  save_interval: 2
  save_path: "checkpoints/rag_cream_enhanced_ppo"
  
  # PPO specific settings
  rollout_buffer_size: 2048    # Size of experience buffer



