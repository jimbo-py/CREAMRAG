# Enhanced DPO Training configuration with Consistency Rewards
device: "cuda"  # Device to use for training

training:
  # Basic parameters - FULL SCALE TRAINING
  epochs: 5  # Full training epochs
  learning_rate: 2e-5  # Optimized for Llama 7B
  batch_size: 8  # Larger batch size for full training
  max_grad_norm: 0.5  # Standard value
  
  # Model parameters
  max_input_length: 2048  # Full sequence length for Llama 7B
  max_new_tokens: 128  # Standard generation length
  temperature: 0.7
  
  # DPO-specific hyperparameters
  dpo_beta: 0.1            # DPO beta parameter
  dpo_method: "original"   # DPO method: original, consistency_avg, consistency_dyn
  
  # Consistency reward settings
  lambda_consistency: 0.5      # Weight for consistency reward
  lambda_retrieval: 0.1        # Weight for retrieval consistency
  consistency_method: "spearman"  # Consistency method: "spearman", "kendall", "toporder"
  num_candidates: 3            # Number of candidates for consistency evaluation
  
  # Training stability settings
  normalize_rewards: true     # Normalize rewards using running statistics
  reward_clipping: true       # Clip rewards to prevent extreme values
  reward_clip_range: 10.0     # Range for reward clipping
  
  # Training strategies
  use_direct_document_training: true
  direct_training_ratio: 0.3
  top_k_variance: true
  
  # Optional: separate questions file
  questions_path: "data/combined/questions.jsonl"
  
  # Data parameters - FULL SCALE TRAINING
  max_questions_from_corpus: 50000  # Use full dataset
  context_length_limit: 1000
  document_truncation_limit: 500
  
  # Optimizer settings
  optimizer: "adamw"  # or "adam"
  weight_decay: 0.01
  scheduler: "cosine"  # or "linear" or null
  
  # Logging and saving - ENHANCED FOR PRODUCTION
  log_interval: 5  # More frequent logging
  eval_interval: 50  # More frequent evaluation
  save_interval: 1  # Save every epoch
  save_path: "/root/CREAMRAG/checkpoints/rag_cream_enhanced_dpo"
  
  # DPO specific settings
  preference_buffer_size: 2048    # Size of preference buffer
  
  # LoRA settings for efficient fine-tuning
  use_lora: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  
  # Quantization settings for memory efficiency
  use_4bit: false
  use_8bit: false
  
  # Flash attention for performance
  use_flash_attention: true
  
  # Checkpointing for research
  save_total_limit: 10  # Keep last 10 checkpoints
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  
  # Logging for research tracking
  report_to: []  # No external logging
  run_name: "dpo_llama7b_cream_consistency"
  
  # DPO settings (already defined above)

# Model configuration
model:
  name: "meta-llama/Llama-2-7b-hf"  # Correct Llama 7B model path
  max_length: 2048
  context_length: 1600  # Increased for RAG context
  
# RAG configuration
rag:
  top_k: 8
  max_doc_length: 1000
  max_context_length: 1600
  use_rag_enhancement: true

# Retriever configuration
retriever:
  document_path: "data/corpus.jsonl"  # Path to document corpus
  index_path: "index_embeddings"  # Path to FAISS index
  top_k: 8
  
# Data sources - ALL DATASETS INCLUDED (FULL SIZE)
datasets:
  - name: "hotpot_qa"
    path: "data/hotpot_qa"
    max_samples: 50  # Small dataset for testing
    
  - name: "natural_questions"
    path: "data/natural_questions" 
    max_samples: 50  # Small dataset for testing
    
  - name: "crag"
    path: "data/crag"
    max_samples: 50  # Small dataset for testing
    
  - name: "squad"
    path: "data/squad"
    max_samples: 50  # Small dataset for testing
    
  - name: "trivia_qa"
    path: "data/trivia_qa"
    max_samples: 50  # Small dataset for testing
    
  - name: "ragbench"
    path: "data/ragbench"
    max_samples: 50  # Small dataset for testing

# Evaluation configuration
evaluation:
  metrics: ["exact_match", "f1_score", "consistency"]
  eval_batch_size: 8
  max_eval_samples: 50

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
# Logging configuration
logging:
  project: "dpo-llama7b-cream"
  entity: null  # Will use default
  tags: ["dpo", "llama7b", "cream", "consistency", "rag"]
  notes: "Full-scale DPO training with Llama 7B, CREAM consistency, and RAG enhancement"



