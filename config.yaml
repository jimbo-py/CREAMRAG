# RAG Cream PPO Training Configuration

# Device configuration
device: "cuda:0"  # or "cpu"

# Retriever configuration
retriever:
  document_path: "data/corpus.jsonl"  # Path to your document corpus
  embedding_path: "index_embeddings/doc_embeddings.npy"  # Path to precomputed embeddings
  top_k: 5  # Number of documents to retrieve

# Generator (LLM) configuration
generator:
  model: "meta-llama/Llama-2-7b-chat-hf"  # Your LLaMA model path
  max_new_tokens: 256
  temperature: 0.7

# Reward model configuration
reward_model:
  model: "meta-llama/Llama-2-7b-chat-hf"  # Can be same as generator

# PPO Training configuration
training:
  # Basic parameters
  epochs: 10
  learning_rate: 1e-5
  batch_size: 4  # PPO batch size (number of prompts to generate at once)
  max_grad_norm: 1.0
  
  # Model parameters
  max_input_length: 1024
  max_new_tokens: 128
  temperature: 0.7
  
  # PPO-specific hyperparameters
  clip_range: 0.2          # PPO clipping parameter
  vf_coef: 0.5             # Value function loss coefficient
  ent_coef: 0.01           # Entropy bonus coefficient
  gamma: 0.99              # Discount factor
  gae_lambda: 0.95         # GAE lambda parameter
  ppo_epochs: 4            # Number of PPO update epochs per batch
  
  # Loss weights (if using additional losses)
  lambda_consistency: 0.1
  lambda_reward: 1.0
  lambda_retrieval: 0.05
  
  # Training strategies
  use_direct_document_training: true
  direct_training_ratio: 0.3
  top_k_variance: true
  
  # Optional: separate questions file
  # questions_path: "data/questions.json"
  
  # Data parameters
  max_questions_from_corpus: 1000
  context_length_limit: 800
  document_truncation_limit: 500
  
  # Optimizer settings
  optimizer: "adamw"  # or "adam"
  weight_decay: 0.01
  scheduler: "cosine"  # or "linear" or null
  
  # Logging and saving
  log_interval: 10
  eval_interval: 100
  save_interval: 2
  save_path: "checkpoints/rag_cream_ppo"
  
  # PPO specific settings
  rollout_buffer_size: 2048    # Size of experience buffer
  minibatch_size: 64           # Minibatch size for PPO updates
  target_kl: 0.01              # Target KL divergence for early stopping
  normalize_advantages: true    # Whether to normalize advantages
  value_loss_clipping: true    # Whether to clip value function loss
